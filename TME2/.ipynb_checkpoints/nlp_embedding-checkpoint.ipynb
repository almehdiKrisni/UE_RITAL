{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 : Neural Embeddings, Text Classification, Text Generation\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **bag of word** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## Dataset\n",
    "https://github.com/cedias/practicalNLP/tree/master/dataset\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (only here for reference)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models\n",
    "6. Pytorch first look: learn to generate text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Loading data (same as in nlp 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../dataset/TME/json_pol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-89cdcee6a46b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Loading json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../dataset/TME/json_pol\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mjson_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../dataset/TME/json_pol'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "#### /!\\ YOU NEED TO UNZIP dataset/json_pol.zip first /!\\\n",
    "\n",
    "\n",
    "# Loading json\n",
    "with open(\"../../dataset/TME2/json_pol\",encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    json_data = json.loads(data[0])\n",
    "    train = json_data[\"train\"]\n",
    "    test = json_data[\"test\"]\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter_train = Counter((x[1] for x in train))\n",
    "counter_test = Counter((x[1] for x in test))\n",
    "print(\"Number of train reviews : \", len(train))\n",
    "print(\"----> # of positive : \", counter_train[1])\n",
    "print(\"----> # of negative : \", counter_train[0])\n",
    "print(\"\")\n",
    "print(train[0])\n",
    "print(\"\")\n",
    "print(\"Number of test reviews : \",len(test))\n",
    "print(\"----> # of positive : \", counter_test[1])\n",
    "print(\"----> # of negative : \", counter_test[0])\n",
    "\n",
    "print(\"\")\n",
    "print(test[0])\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: train (or load) a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 17:06:31,792 : INFO : collecting all words and their counts\n",
      "2022-02-03 17:06:31,792 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-02-03 17:06:32,097 : INFO : PROGRESS: at sentence #10000, processed 2358544 words, keeping 155393 word types\n",
      "2022-02-03 17:06:32,413 : INFO : PROGRESS: at sentence #20000, processed 4675912 words, keeping 243050 word types\n",
      "2022-02-03 17:06:32,578 : INFO : collected 280617 word types from a corpus of 5844680 raw words and 25000 sentences\n",
      "2022-02-03 17:06:32,578 : INFO : Creating a fresh vocabulary\n",
      "2022-02-03 17:06:32,748 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 49345 unique words (17.584465659600095%% of original 280617, drops 231272)', 'datetime': '2022-02-03T17:06:32.748640', 'gensim': '4.0.1', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-02-03 17:06:32,749 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 5517507 word corpus (94.40220850414394%% of original 5844680, drops 327173)', 'datetime': '2022-02-03T17:06:32.749106', 'gensim': '4.0.1', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-02-03 17:06:32,919 : INFO : deleting the raw counts dictionary of 280617 items\n",
      "2022-02-03 17:06:32,923 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2022-02-03 17:06:32,923 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 4268608.194985565 word corpus (77.4%% of prior 5517507)', 'datetime': '2022-02-03T17:06:32.923857', 'gensim': '4.0.1', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'prepare_vocab'}\n",
      "2022-02-03 17:06:33,253 : INFO : estimated required memory for 49345 words and 100 dimensions: 64148500 bytes\n",
      "2022-02-03 17:06:33,253 : INFO : resetting layer weights\n",
      "2022-02-03 17:06:33,277 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-02-03T17:06:33.277011', 'gensim': '4.0.1', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'build_vocab'}\n",
      "2022-02-03 17:06:33,277 : INFO : Word2Vec lifecycle event {'msg': 'training model with 3 workers on 49345 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2022-02-03T17:06:33.277506', 'gensim': '4.0.1', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-02-03 17:06:34,284 : INFO : EPOCH 1 - PROGRESS: at 8.98% examples, 390079 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:35,305 : INFO : EPOCH 1 - PROGRESS: at 18.61% examples, 394531 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:36,330 : INFO : EPOCH 1 - PROGRESS: at 28.06% examples, 395833 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:37,343 : INFO : EPOCH 1 - PROGRESS: at 37.20% examples, 393858 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:38,382 : INFO : EPOCH 1 - PROGRESS: at 47.23% examples, 397979 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:39,417 : INFO : EPOCH 1 - PROGRESS: at 57.34% examples, 400939 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:40,440 : INFO : EPOCH 1 - PROGRESS: at 67.55% examples, 403926 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:41,452 : INFO : EPOCH 1 - PROGRESS: at 77.72% examples, 406019 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:42,464 : INFO : EPOCH 1 - PROGRESS: at 87.48% examples, 407037 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:43,483 : INFO : EPOCH 1 - PROGRESS: at 96.95% examples, 406039 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:43,760 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-03 17:06:43,764 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-03 17:06:43,775 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-03 17:06:43,775 : INFO : EPOCH - 1 : training on 5844680 raw words (4269296 effective words) took 10.5s, 406712 effective words/s\n",
      "2022-02-03 17:06:44,796 : INFO : EPOCH 2 - PROGRESS: at 9.51% examples, 406876 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:45,820 : INFO : EPOCH 2 - PROGRESS: at 19.59% examples, 412899 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:46,823 : INFO : EPOCH 2 - PROGRESS: at 29.21% examples, 413279 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:47,852 : INFO : EPOCH 2 - PROGRESS: at 39.29% examples, 414191 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:48,883 : INFO : EPOCH 2 - PROGRESS: at 49.18% examples, 414758 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:49,910 : INFO : EPOCH 2 - PROGRESS: at 59.40% examples, 415499 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:50,935 : INFO : EPOCH 2 - PROGRESS: at 69.65% examples, 416233 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:51,938 : INFO : EPOCH 2 - PROGRESS: at 79.83% examples, 417353 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:52,956 : INFO : EPOCH 2 - PROGRESS: at 89.57% examples, 416799 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:53,968 : INFO : EPOCH 2 - PROGRESS: at 99.26% examples, 415850 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:54,012 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-03 17:06:54,027 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-03 17:06:54,031 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-03 17:06:54,032 : INFO : EPOCH - 2 : training on 5844680 raw words (4268942 effective words) took 10.3s, 416339 effective words/s\n",
      "2022-02-03 17:06:55,047 : INFO : EPOCH 3 - PROGRESS: at 8.98% examples, 387623 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:56,079 : INFO : EPOCH 3 - PROGRESS: at 18.61% examples, 391325 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:57,102 : INFO : EPOCH 3 - PROGRESS: at 28.06% examples, 393909 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:58,107 : INFO : EPOCH 3 - PROGRESS: at 37.39% examples, 395033 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:06:59,125 : INFO : EPOCH 3 - PROGRESS: at 46.72% examples, 394913 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:00,140 : INFO : EPOCH 3 - PROGRESS: at 56.72% examples, 398560 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:01,144 : INFO : EPOCH 3 - PROGRESS: at 66.50% examples, 400814 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:02,172 : INFO : EPOCH 3 - PROGRESS: at 76.66% examples, 402593 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:03,183 : INFO : EPOCH 3 - PROGRESS: at 86.64% examples, 404786 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:04,208 : INFO : EPOCH 3 - PROGRESS: at 96.44% examples, 405217 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:04,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-03 17:07:04,528 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-03 17:07:04,537 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-03 17:07:04,538 : INFO : EPOCH - 3 : training on 5844680 raw words (4269431 effective words) took 10.5s, 406506 effective words/s\n",
      "2022-02-03 17:07:05,562 : INFO : EPOCH 4 - PROGRESS: at 9.51% examples, 404611 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:06,587 : INFO : EPOCH 4 - PROGRESS: at 19.59% examples, 411539 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:07,615 : INFO : EPOCH 4 - PROGRESS: at 29.59% examples, 413491 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:08,623 : INFO : EPOCH 4 - PROGRESS: at 39.48% examples, 414843 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:09,629 : INFO : EPOCH 4 - PROGRESS: at 49.04% examples, 414565 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:10,645 : INFO : EPOCH 4 - PROGRESS: at 58.92% examples, 413785 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:11,663 : INFO : EPOCH 4 - PROGRESS: at 68.92% examples, 414102 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:12,689 : INFO : EPOCH 4 - PROGRESS: at 79.16% examples, 414382 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 17:07:13,705 : INFO : EPOCH 4 - PROGRESS: at 88.92% examples, 414309 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:14,731 : INFO : EPOCH 4 - PROGRESS: at 98.91% examples, 414401 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:14,810 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-03 17:07:14,822 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-03 17:07:14,844 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-03 17:07:14,844 : INFO : EPOCH - 4 : training on 5844680 raw words (4269629 effective words) took 10.3s, 414307 effective words/s\n",
      "2022-02-03 17:07:15,874 : INFO : EPOCH 5 - PROGRESS: at 9.51% examples, 402704 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:16,876 : INFO : EPOCH 5 - PROGRESS: at 19.43% examples, 411679 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:17,892 : INFO : EPOCH 5 - PROGRESS: at 29.03% examples, 410823 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:18,892 : INFO : EPOCH 5 - PROGRESS: at 38.73% examples, 411802 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:19,905 : INFO : EPOCH 5 - PROGRESS: at 48.51% examples, 412817 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:20,923 : INFO : EPOCH 5 - PROGRESS: at 58.38% examples, 412242 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:21,935 : INFO : EPOCH 5 - PROGRESS: at 68.37% examples, 413154 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:22,937 : INFO : EPOCH 5 - PROGRESS: at 78.29% examples, 412927 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:23,946 : INFO : EPOCH 5 - PROGRESS: at 87.48% examples, 410897 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:24,960 : INFO : EPOCH 5 - PROGRESS: at 96.95% examples, 409703 words/s, in_qsize 5, out_qsize 0\n",
      "2022-02-03 17:07:25,229 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2022-02-03 17:07:25,232 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2022-02-03 17:07:25,242 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2022-02-03 17:07:25,242 : INFO : EPOCH - 5 : training on 5844680 raw words (4269660 effective words) took 10.4s, 410690 effective words/s\n",
      "2022-02-03 17:07:25,242 : INFO : Word2Vec lifecycle event {'msg': 'training on 29223400 raw words (21346958 effective words) took 52.0s, 410790 effective words/s', 'datetime': '2022-02-03T17:07:25.242779', 'gensim': '4.0.1', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'train'}\n",
      "2022-02-03 17:07:25,243 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=49345, vector_size=100, alpha=0.025)', 'datetime': '2022-02-03T17:07:25.243000', 'gensim': '4.0.1', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.split() for t,p in train]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                vector_size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1,\n",
    "                                epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 16:59:01,090 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2022-02-03 16:59:01,091 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2022-02-03 16:59:01,092 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\", 'datetime': '2022-02-03T16:59:01.092617', 'gensim': '4.0.1', 'python': '3.9.7 (default, Sep 16 2021, 08:50:36) \\n[Clang 10.0.0 ]', 'platform': 'macOS-10.16-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'KeyedVectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ch/hpfc1rn52_b8zlz4r9mh7bth0000gn/T/ipykernel_10898/3555840456.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatapath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mw2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'downloaded_vectors_path'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KeyedVectors' is not defined"
     ]
    }
   ],
   "source": [
    "# It's for later\n",
    "\n",
    "#from gensim.test.utils import datapath\n",
    "#w2v = KeyedVectors.load_word2vec_format(datapath('downloaded_vectors_path'), binary=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great and good: 0.7806154\n",
      "great and bad: 0.47475743\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.9363027811050415),\n",
       " ('\"film\"', 0.8453736305236816),\n",
       " ('programme', 0.7757548689842224),\n",
       " ('\"movie\"', 0.7721199989318848),\n",
       " ('movie,', 0.7674430012702942)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "w2v.wv.most_similar(\"movie\",topn=5) # 5 most similar words\n",
    "#w2v.wv.most_similar(\"awesome\",topn=5)\n",
    "#w2v.wv.most_similar(\"actor\",topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.8106686472892761),\n",
       " ('actress,', 0.7266996502876282),\n",
       " ('actress.', 0.6623290181159973)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3)  \n",
    "\n",
    "w2v.wv.most_similar(positive=[\"actor\",\"woman\"],negative=[\"man\"],topn=3) # do the famous exemple works for actor ?\n",
    "\n",
    "\n",
    "# Try other things like plurals for exemple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 17:19:06,695 : INFO : Evaluating word analogies for top 300000 words in the model on dataset/questions-words.txt\n",
      "2022-02-03 17:19:07,912 : INFO : capital-common-countries: 0.6% (1/156)\n",
      "2022-02-03 17:19:08,757 : INFO : capital-world: 0.0% (0/111)\n",
      "2022-02-03 17:19:08,787 : INFO : currency: 0.0% (0/18)\n",
      "2022-02-03 17:19:09,326 : INFO : city-in-state: 0.0% (0/301)\n",
      "2022-02-03 17:19:16,546 : INFO : family: 31.0% (130/420)\n",
      "2022-02-03 17:19:31,916 : INFO : gram1-adjective-to-adverb: 1.8% (16/870)\n",
      "2022-02-03 17:19:35,581 : INFO : gram2-opposite: 1.8% (10/552)\n",
      "2022-02-03 17:19:50,295 : INFO : gram3-comparative: 20.1% (239/1190)\n",
      "2022-02-03 17:20:04,104 : INFO : gram4-superlative: 9.8% (74/756)\n",
      "2022-02-03 17:20:08,916 : INFO : gram5-present-participle: 19.2% (156/812)\n",
      "2022-02-03 17:20:22,194 : INFO : gram6-nationality-adjective: 1.7% (16/967)\n",
      "2022-02-03 17:20:44,224 : INFO : gram7-past-tense: 21.0% (265/1260)\n",
      "2022-02-03 17:20:51,268 : INFO : gram8-plural: 9.0% (73/812)\n",
      "2022-02-03 17:21:03,615 : INFO : gram9-plural-verbs: 31.1% (202/650)\n",
      "2022-02-03 17:21:03,616 : INFO : Quadruplets with out-of-vocabulary words: 54.6%\n",
      "2022-02-03 17:21:03,617 : INFO : NB: analogies containing OOV words were skipped from evaluation! To change this behavior, use \"dummy4unknown=True\"\n",
      "2022-02-03 17:21:03,617 : INFO : Total accuracy: 13.3% (1182/8875)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.evaluate_word_analogies(\"dataset/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.key_to_index` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49345"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test - key_to_index size\n",
    "len(w2v.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 1, 'undoubted': 1, 'highlight': 1, 'of': 1, 'this': 1, 'movie': 1, 'is': 3, 'Peter': 2, \"O'Toole's\": 2, 'performance.': 1, 'In': 1, 'turn': 1, 'wildly': 1, 'comical': 1, 'and': 5, 'terribly': 2, 'tragic.': 1, 'Does': 1, 'anybody': 1, 'do': 1, 'it': 3, 'better': 1, 'than': 1, \"O'Toole?\": 1, 'I': 3, \"don't\": 1, 'think': 1, 'so.': 1, 'What': 1, 'a': 1, 'great': 1, 'face': 1, 'that': 1, 'man': 1, 'has!<br': 1, '/><br': 1, '/>The': 1, 'story': 1, 'an': 1, 'odd': 1, 'one': 1, 'quite': 1, 'disturbing': 1, 'emotionally': 1, 'intense': 1, 'in': 2, 'parts': 1, '(especially': 1, 'toward': 1, 'the': 2, 'end)': 1, 'but': 1, 'also': 1, 'oddly': 1, 'touching': 1, 'does': 1, 'succeed': 1, 'on': 1, 'many': 1, 'levels.': 1, 'However,': 1, 'felt': 1, 'film': 1, 'basically': 1, 'revolved': 1, 'around': 1, 'luminous': 1, 'performance': 1, \"I'm\": 1, 'sure': 1, \"wouldn't\": 1, 'have': 1, 'enjoyed': 1, 'even': 1, 'half': 1, 'as': 1, 'much': 1, 'if': 1, 'he': 1, \"hadn't\": 1, 'been': 1, 'it.': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "\n",
    "def vectorize(text,mean=False):\n",
    "    \"\"\"\n",
    "    This function should vectorize one review\n",
    "\n",
    "    input: str\n",
    "    output: np.array(float)\n",
    "    \"\"\"    \n",
    "    \n",
    "    # We use a dict structure to vectorize a review\n",
    "    vec = dict()\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    for word in words :\n",
    "        if word not in vec.keys() :\n",
    "            vec[word] = 1\n",
    "        else :\n",
    "            vec[word] += 1\n",
    "            \n",
    "    return vec\n",
    "    \n",
    "\n",
    "classes = [pol for text,pol in train]\n",
    "X = [vectorize(text) for text,pol in train]\n",
    "X_test = [vectorize(text) for text,pol in test]\n",
    "true = [pol for text,pol in test]\n",
    "\n",
    "#let's see what a review vector looks like.\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Scikit Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Generate text with a recurrent neural network (Pytorch) ---\n",
    "### (Mostly Read & Run)\n",
    "\n",
    "The goal is to replicate the (famous) experiment from [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "To learn to generate text, we train a recurrent neural network to do the following task:\n",
    "\n",
    "Given a \"chunk\" of text: `this is random text`\n",
    "\n",
    "the goal of the network is to predict each character in **`his is random text` ** sequentially given the following sequential input **`this is random tex`**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input ->  Output\n",
    "--------------\n",
    "T    ->    H\n",
    "H    ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "\" \"  ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "[...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load text (dataset/input.txt)\n",
    "\n",
    "Before building training batch, we load the full text in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('dataset/input.txt').read()) #clean text => only ascii\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Helper functions:\n",
    "\n",
    "We have a text and we want to feed batch of chunks to a neural network:\n",
    "\n",
    "one chunk  A,B,C,D,E\n",
    "[input] A,B,C,D -> B,C,D,E [output]\n",
    "\n",
    "Note: we will use an embedding layer instead of a one-hot encoding scheme.\n",
    "\n",
    "for this, we have 3 functions:\n",
    "\n",
    "- One to get a random str chunk of size `chunk_len` : `random_chunk` \n",
    "- One to turn a chunk into a tensor of size `(1,chunk_len)` coding for each characters : `char_tensor`\n",
    "- One to return random input and output chunks of size `(batch_size,chunk_len)` : `random_training_set`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "\n",
    "#Get a piece of text\n",
    "def random_chunk(chunk_len):\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(1,len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[0,c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#Turn a piece of text in train/test\n",
    "def random_training_set(chunk_len=200, batch_size=8):\n",
    "    chunks = [random_chunk(chunk_len) for _ in range(batch_size)]\n",
    "    inp = torch.cat([char_tensor(chunk[:-1]) for chunk in chunks],dim=0)\n",
    "    target = torch.cat([char_tensor(chunk[1:]) for chunk in chunks],dim=0)\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "print(random_training_set(10,4))  ## should return 8 chunks of 10 letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual RNN model (only thing to complete):\n",
    "\n",
    "It should be composed of three distinct modules:\n",
    "\n",
    "- an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) (n_characters, hidden_size)\n",
    "\n",
    "```\n",
    "nn.Embedding(len_dic,size_vec)\n",
    "```\n",
    "- a [recurrent](https://pytorch.org/docs/stable/nn.html#recurrent-layers) layer (hidden_size, hidden_size)\n",
    "```\n",
    "nn.RNN(in_size,out_size) or nn.GRU() or nn.LSTM() => rnn_cell parameter\n",
    "```\n",
    "- a [prediction](https://pytorch.org/docs/stable/nn.html#linear) layer (hidden_size, output_size)\n",
    "\n",
    "```\n",
    "nn.Linear(in_size,out_size)\n",
    "```\n",
    "=> Complete the `init` function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_char, hidden_size, output_size, n_layers=1,rnn_cell=nn.RNN):\n",
    "        \"\"\"\n",
    "        Create the network\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_char = n_char\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #  (batch,chunk_len) -> (batch, chunk_len, hidden_size)  \n",
    "        self.embed = ####\n",
    "        \n",
    "        # (batch, chunk_len, hidden_size)  -> (batch, chunk_len, hidden_size)  \n",
    "        self.rnn = ####\n",
    "        \n",
    "        #(batch, chunk_len, hidden_size) -> (batch, chunk_len, output_size)  \n",
    "        self.predict = ####\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        batched forward: input is (batch > 1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,_  = self.rnn(input)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output\n",
    "    \n",
    "    def forward_seq(self, input,hidden=None):\n",
    "        \"\"\"\n",
    "        not batched forward: input is  (1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,hidden  = self.rnn(input.unsqueeze(0),hidden)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output,hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text generation function\n",
    "\n",
    "Sample text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate(model,prime_str='A', predict_len=100, temperature=0.8):\n",
    "    prime_input = char_tensor(prime_str).squeeze(0)\n",
    "    hidden = None\n",
    "    predicted = prime_str+\"\"\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "\n",
    "    for p in range(len(prime_str)-1):\n",
    "        _,hidden = model.forward_seq(prime_input[p].unsqueeze(0),hidden)\n",
    "            \n",
    "    #print(hidden.size())\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model.forward_seq(prime_input[-1].unsqueeze(0), hidden)\n",
    "                # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        #print(output_dist)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        #print(top_i)\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        prime_input = torch.cat([prime_input,char_tensor(predicted_char).squeeze(0)])\n",
    "\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loop for net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "###Parameters\n",
    "n_epochs = 20000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 5\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "chunk_len = 80\n",
    "\n",
    "####\n",
    "\n",
    "model = RNN(n_characters, hidden_size, n_characters, n_layers) #create model\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=lr) #create Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different temperatures\n",
    "\n",
    "Changing the distribution sharpness has an impact on character sampling:\n",
    "\n",
    "more or less probable things are sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(generate(model,'T', 200, temperature=1))\n",
    "print(\"----\")\n",
    "print(generate(model,'Th', 200, temperature=0.8))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.5))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.3))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving this code:\n",
    "\n",
    "(a) Tinker with parameters:\n",
    "\n",
    "- Is it really necessary to have 100 dims character embeddings\n",
    "- Chunk length can be gradually increased\n",
    "- Try changing RNN cell type (GRUs - LSTMs)\n",
    "\n",
    "(b) Add GPU support to go faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ End of practical\n",
    "\n",
    "#### Legacy loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from os.path import split as pathsplit\n",
    "\n",
    "dir_train = \"dataset/aclImdb/train/\"\n",
    "dir_test = \"dataset/aclImdb/test/\"\n",
    "\n",
    "train_files = glob.glob(dir_train+'pos/*.txt') + glob.glob(dir_train+'neg/*.txt')\n",
    "test_files = glob.glob(dir_test+'pos/*.txt') + glob.glob(dir_test+'neg/*.txt')\n",
    "\n",
    "\n",
    "def get_polarity(f):\n",
    "    \"\"\"\n",
    "    Extracts polarity from filename:\n",
    "    0 is negative (< 5)\n",
    "    1 is positive (> 5)\n",
    "    \"\"\"\n",
    "    _,name = pathsplit(f)\n",
    "    if int(name.split('_')[1].split('.')[0]) < 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def open_one(f):\n",
    "    \n",
    "    polarity = get_polarity(f)\n",
    "    \n",
    "    with open(f,\"r\") as review:\n",
    "        text = \" \".join(review.readlines()).strip()\n",
    "    \n",
    "    return (text,polarity)\n",
    "\n",
    "print(open_one(train_files[0]))\n",
    "\n",
    "train = [open_one(x) for x in train_files] #contains (text,pol) couples\n",
    "test = [open_one(x) for x in test_files]   #contains (text,pol) couples\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
