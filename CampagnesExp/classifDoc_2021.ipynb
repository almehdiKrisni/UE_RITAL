{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de documents : prise en main des outils\n",
    "\n",
    "Le but de ce TP est de classer des documents textuels... Dans un premier temps, nous allons vérifier le bon fonctionnement des outils sur des données jouets puis appliquer les concepts sur des données réelles.\n",
    "\n",
    "\n",
    "## Conception de la chaine de traitement\n",
    "Pour rappel, une chaine de traitement de documents classique est composée des étapes suivantes:\n",
    "1. Lecture des données et importation\n",
    "    - Dans le cadre de nos TP, nous faisons l'hypothèse que le corpus tient en mémoire... Si ce n'est pas le cas, il faut alors ajouter des structures de données avec des buffers (*data-reader*), bien plus complexes à mettre en place.\n",
    "    - Le plus grand piège concerne l'encodage des données. Dans le TP... Pas (ou peu) de problème. Dans la vraie vie: il faut faire attention à toujours maitriser les formats d'entrée et de sortie.\n",
    "1. Traitement des données brutes paramétrique. Chaque traitement doit être activable ou desactivable + paramétrable si besoin.\n",
    "    - Enlever les informations *inutiles* : chiffre, ponctuations, majuscules, etc... <BR>\n",
    "    **L'utilité dépend de l'application!**\n",
    "    - Segmenter en mots (=*Tokenization*)\n",
    "    - Elimination des stop-words\n",
    "    - Stemming/lemmatisation (racinisation)\n",
    "    - Byte-pair encoding pour trouver les mots composés (e.g. Sorbonne Université, Ville de Paris, Premier Ministre, etc...)\n",
    "1. Traitement des données numériques\n",
    "    - Normalisation *term-frequency* / binarisation\n",
    "    - Normalisation *inverse document frequency*\n",
    "    - Elimination des mots rares, des mots trop fréquents\n",
    "    - Construction de critère de séparabilité pour éliminer des mots etc...\n",
    "1. Apprentissage d'un classifieur\n",
    "    - Choix du type de classifieur\n",
    "    - Réglage des paramètres du classifieur (régularisation, etc...)\n",
    "\n",
    "## Exploitation de la chaine de traitement\n",
    "\n",
    "On appelle cette étape la réalisation d'une campagne d'expériences: c'est le point clé que nous voulons traviller en TAL cette année.\n",
    "1. Il est impossible de tester toutes les combinaisons par rapport aux propositions ci-dessus... Il faut donc en éliminer un certain nombre.\n",
    "    - En discutant avec les experts métiers\n",
    "    - En faisant des tests préliminaires\n",
    "1. Après ce premier filtrage, il faut:\n",
    "    - Choisir une évaluation fiable et pas trop lente (validation croisée, leave-one-out, split apprentissage/test simple)\n",
    "    - Lancer des expériences en grand\n",
    "        - = *grid-search*\n",
    "        - parallèliser sur plusieurs machines\n",
    "        - savoir lancer sur un serveur et se déconnecter\n",
    "1. Collecter et analyser les résultats\n",
    "\n",
    "\n",
    "## Inférence\n",
    "\n",
    "L'inférence est ensuite très classique: la chaine de traitement optimale est apte à traiter de nouveaux documents\n",
    "\n",
    "# Etape 1: charger les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données:\n",
    "def load_pres(fname):\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(-1)\n",
    "        else: \n",
    "            alllabs.append(1)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../../dataset/projetTAL/AFDpresidentutf8/corpus.tache1.learn.utf8\"\n",
    "\n",
    "alltxts,alllabs = load_pres(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57413 57413\n",
      " Quand je dis chers amis, il ne s'agit pas là d'une formule diplomatique, mais de l'expression de ce que je ressens.\n",
      "\n",
      "1\n",
      " Je compte sur vous.\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(alltxts),len(alllabs))\n",
    "print(alltxts[0])\n",
    "print(alllabs[0])\n",
    "print(alltxts[-1])\n",
    "print(alllabs[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_movies(path2data): # 1 classe par répertoire\n",
    "    alltxts = [] # init vide\n",
    "    labs = []\n",
    "    cpt = 0\n",
    "    for cl in os.listdir(path2data): # parcours des fichiers d'un répertoire\n",
    "        for f in os.listdir(path2data+cl):\n",
    "            txt = open(path2data+cl+'/'+f).read()\n",
    "            alltxts.append(txt)\n",
    "            labs.append(cpt)\n",
    "        cpt+=1 # chg répertoire = cht classe\n",
    "        \n",
    "    return alltxts,labs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../dataset/projetTAL/movies1000/\"\n",
    "\n",
    "alltxts,alllabs = load_movies(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "bad . bad . \n",
      "bad . \n",
      "that one word seems to pretty much sums up beyond the valley of the dolls . \n",
      "if that summary isn't enough for you , how about t&a , t&a , t&a ? \n",
      "still haven't got the point ? \n",
      "other than director russ meyer's predilection for casting attractive large breasted women who ultimately expose the afore-mentioned anatomical areas , there is really only one other reason to recommend even taking a look at this movie . \n",
      "that is the fact that it was co-written by famed film critic roger ebert , who also was responsible for the screenplay . \n",
      "after watching this movie you will never be able to sit through another one of his reviews where he gives a movie a thumbs down for bad writing with a straight face . \n",
      "this movie stinks out loud . \n",
      "quite frankly , this movie deserves a . \n",
      "but there are parts of it that are so bad they are almost funny . \n",
      "so i'm giving it a . \n",
      "and maybe that is too generous . \n",
      "right from the opening credits , i knew that i had a class-a bomb on my hands . \n",
      "not only are the way the credits actually shot distracting , but the first scene you see includes a big breasted young woman being chased by a guy in a nazi uniform . \n",
      "i had absolutely no idea why the hell that was happening ( it does get explained later ) and as soon as the first scene is over , we cut to a completely unrelated scene . \n",
      "to be honest , as i sat through this movie mesmerized by just how incredibly awful it was , i actually forgot about the seemingly out of place opening until it popped up again later in the film . \n",
      "with the quality of the writing during the rest of the film , it wouldn't have surprised me if the opening had never been explained . \n",
      "so what is this movie about ? \n",
      "you ask . \n",
      "like it really matters . \n",
      "ok , here goes . \n",
      "this all-girl band headed by kelly macnamara ( dolly reed ) and her friends go to hollywood to try to gain a foothold in the music industry . \n",
      "once there , they do manage to find success ( due as much to their hooters as anything else ? it sure wasn't for their brutally bad singing voices ) , and the movie chronicles how their lives change for the worse as the pressures of fame get to them . \n",
      "everything from big egos , to booze and drugs to free flowing sex sends them on a downward spiral . \n",
      "there are a couple of other idiotic subplots thrown in for good measure , but the fame is the one that pretty much sums up this thing . \n",
      ">from a creative standpoint there is nothing redeeming here . \n",
      "other than the above-mentioned obsession with big knockers that russ meyer seemed to have . \n",
      "the dialogue is so incredibly bad that it literally is funny in parts . \n",
      "mr . ebert has generously thrown in helpings of \" hey man \" , \" dig \" and my all time favorite -- \" this is my happening , and it freaks me out \" . \n",
      "now i ask you , with lines like that how can you go wrong ? \n",
      "ebert had tried to inject as many big words as possible into the dialogue . \n",
      "maybe he thought it would make the movie seem smarter . \n",
      "i don't know , but all the big words in the world wouldn't be able to disguise the bad writing and even worse acting . \n",
      "but the wretched dialogue goes along well with the wretched quality of everything else in this movie . \n",
      "i've seen home movies directed better than meyer managed with this turkey . \n",
      "in fact , there is one scene -- the one in which they are in a van driving to hollywood to make their fortunes -- during which i really had to question if meyer or his editors had just suffered serious head injuries . \n",
      "add to the directing and writing the music in this movie . \n",
      "i almost got up to check my sound system to see if it was broken , there was such a pile of crap emanating from the speakers . \n",
      "then we have the cast . \n",
      "first lets start david gurian who played harris , the manager of the band . \n",
      "this has got to be the goofiest looking guy that has ever set foot in front of a motion picture camera . \n",
      "sadly , his acting doesn't come close to making up for his looks . \n",
      "if you have been following along up to this point , this shouldn't surprise you . \n",
      "meyer's stable of well endowed girls also have the benefit of being fairly attractive to go along with their other assets . \n",
      "dolly reed plays kelly , the leader of the band . \n",
      "and no surprise here , she was cast for her cup size , not her talents . \n",
      "and yes , she does loose the shirt a few times and display her impressive talents . \n",
      "sadly , her ass is almost as large as her chest . \n",
      "hey , it a sexist movie , so i'm writing a sexist review . \n",
      "then we have former playboy playmate cynthia myers in a fairly small role as casey , one of the other band members . \n",
      "this goes along with the rest of the idiotic thinking in the movie . \n",
      "meyer casts a gorgeous playmate with a rack to kill for and who obviously has no acting talent at all , but her nude scenes are the biggest disappointment of all . \n",
      "sure russ , now is the time to get artsy and throw in some well placed shadows . \n",
      "on the up side , she does have a fun lesbo scene . \n",
      "i sound like i'm writing a review in a porn magazine . \n",
      "but hey , i'll admit it ; the only reason that i actually managed to sit through this damn movie was to catch a look at cynthia myers naked . \n",
      "and since that was a huge disappointment , i pretty much wasted two hours of my life on this turkey . \n",
      "the only thing that i can say about this movie is that you should stay away from it . \n",
      "unless of course you want to feel good about yourself by knowing that even a pulitzer prize winning film critic like roger ebert has screwed up at least once in his life too . \n",
      "and if you are thinking of checking it out for the double d's -- you are better off just downloading nude cynthia myers pictures off the internet . \n",
      "this is a movie that should be avoided at all costs . \n",
      "an even better idea might be to require video stores to place a warning on the box of beyond the valley of the dolls -- beware : this movie is extremely hazardous to your common sense . \n",
      "proceed with extreme caution . \n",
      "\n",
      "0\n",
      "\n",
      "#######################################\n",
      "\n",
      "\n",
      "capsule : the director of cure brings a weird and very complex concept to the screen . \n",
      "one viewing will not be enough to understand fully the premise of pulse . \n",
      "the idea is something about ghosts and the internet . \n",
      "the film has an amazing apocalyptic style . \n",
      " , +2 ( -4 to +4 ) \n",
      "perhaps the most disturbing ( and disturbed ? ) \n",
      "filmmaker in the world is kiyoshi kurosawa . \n",
      "his films all seem to have one style , bleak . \n",
      "the worlds he creates are terrifying and cold . \n",
      "little known in the us to date , his films deliver the kind of horror that so many of our filmmakers promise and are unable to deliver . \n",
      "most of his ideas are fresh and at the same time morbid . \n",
      "his 1998 film cure , with one of his niftiest ideas , is just now getting a sadly limited release in the us and hopefully enough people will see it that his name will soon be one to conjure with . \n",
      "cure is probably his classic . \n",
      "last year he released seance , a remake of seance on a wet afternoon . \n",
      "that was perhaps a miscalculation inserting supernatural elements into a non-supernatural story . \n",
      "pulse is kurosawa back on form . \n",
      "taguchi , a young computer expert , is late with his delivery of some important software . \n",
      "two co-workers go to his apartment and find it a dismal dark affair in spite of his computer equipment . \n",
      "taguchi , acting very strangely , lets his friends look for the missing software . \n",
      "meanwhile he slips behind a plastic curtain . \n",
      "when he fails to respond to calls his friends follow him behind the curtain and discover he has hanged himself . \n",
      "if that was not horror enough the body seems to disappear leaving just a strange dark mildew-like spot on the wall . \n",
      "taguchi's computer seems to have been infected with some kind of computer virus . \n",
      "people whose computer gets the virus seem superficially to die via suicide . \n",
      "but they are not entirely dead . \n",
      "their spirits seem to remain present somehow in the real world and on the internet . \n",
      "people who get the computer virus are asked if they want to see a ghost . \n",
      "if they say yes , they seem to be able to see real time images of the spirits still nearby somehow . \n",
      "the computer shows them impossible images of ghosts in their own rooms as seen from cameras that do not exist . \n",
      "this is all somehow connected to heaven and hell somehow filling up and overflowing \" like a computer disk . \" \n",
      "instead the dead seem to be staying on earth and inhabiting computer viruses . \n",
      "there is some sort of passage between worlds having something to do with doors marked with red tape and strange electronic disturbances on computers . \n",
      "leave it to kurosawa to find a new kind of death . \n",
      "this is a film that has more weird ideas piled together than lifeforce and somehow kurosawa makes the film all work . \n",
      "it may not totally convey his message of isolation and its parallels to death , but whatever it does convey is nightmarish . \n",
      "kurosawa , who directs his own screenplay , ties his story into the real world with some familiar and accurate computer discussion . \n",
      "frequently the plot is advanced with character hunches being assumed to be fact . \n",
      "his plotting is frequently hard to follow and always very strange . \n",
      "junichiro hayashi , the cinematographer who recently has been doing all of kurosawa's films , creates a dark , cold , and gloomy tone . \n",
      "images are obscured by semi-lighting or are behind plastic curtain . \n",
      "scenes are not milked for their horror the way american exploitation films might . \n",
      "people are shot with guns but there is little if any blood in evidence . \n",
      "seeing black silhouettes on computer screens is not immediately scary . \n",
      "kurosawa is not going for and easy visual shock , but a deeper metaphysical dread . \n",
      "of any horror filmmaker in the world , kiyoshi kurosawa is the one to watch . \n",
      "i rate this metaphysical look at isolation a 7 on the 0 to 10 scale and a +2 on the -4 to +4 scale . \n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(alltxts),len(alllabs))\n",
    "print(alltxts[0])\n",
    "print(alllabs[0])\n",
    "print(\"\\n#######################################\\n\\n\")\n",
    "print(alltxts[-1])\n",
    "print(alllabs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation paramétrique du texte\n",
    "\n",
    "Vous devez tester, par exemple, les cas suivants:\n",
    "- transformation en minuscule ou pas\n",
    "- suppression de la ponctuation\n",
    "- transformation des mots entièrement en majuscule en marqueurs spécifiques\n",
    "- suppression des chiffres ou pas\n",
    "- conservation d'une partie du texte seulement (seulement la première ligne = titre, seulement la dernière ligne = résumé, ...)\n",
    "- stemming\n",
    "- ...\n",
    "\n",
    "\n",
    "Vérifier systématiquement sur un exemple ou deux le bon fonctionnement des méthodes sur deux documents (au moins un de chaque classe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import *\n",
    "\n",
    "def transform(text, maj=False, chiffres=False) :\n",
    "    text_transf = text\n",
    "    \n",
    "    for i in range(len(text)) :\n",
    "            \n",
    "            # Vérification des majuscules\n",
    "            if maj :\n",
    "                text_transf[i] = text_transf[i].lower()\n",
    "                \n",
    "            # Vérification de la ponctuation\n",
    "            text_transf[i] = text_transf[i].translate(str.maketrans('', '', string.punctuation))\n",
    "            \n",
    "            # Vérification des chiffres\n",
    "            if chiffres :\n",
    "                text_transf[i] = ''.join([j for j in text_transf[i] if not j.isdigit()])\n",
    "            \n",
    "    return text_transf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['helllllllo my friend', 'stupid']\n"
     ]
    }
   ],
   "source": [
    "print(transform([\"heLLLLLLlo11114986493,,,, My Friend\", \"StupiD\"], maj=True, ponctuation=True, chiffres=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction du vocabulaire\n",
    "\n",
    "Exploration préliminaire des jeux de données.\n",
    "\n",
    "- Quelle est la taille d'origine du vocabulaire?\n",
    "- Que reste-t-il si on ne garde que les 100 mots les plus fréquents? [word cloud]\n",
    "- Quels sont les 100 mots dont la fréquence documentaire est la plus grande? [word cloud]\n",
    "- Quels sont les 100 mots les plus discriminants au sens de odds ratio? [word cloud]\n",
    "- Quelle est la distribution d'apparition des mots (Zipf)\n",
    "- Quels sont les 100 bigrammes/trigrammes les plus fréquents?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction() :\n",
    "    \n",
    "    # Taille d'origine du vocabulaire\n",
    "    \n",
    "    # 100 mots les plus fréquents\n",
    "    \n",
    "    # 100 mots à la fréquence documentaire la plus grande\n",
    "    \n",
    "    # 100 mots les plus discriminants au sens de odds ratio\n",
    "    \n",
    "    # Distribution d'apparition des mots\n",
    "    \n",
    "    # Bigrammes et trigrammes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question qui devient de plus en plus intéressante avec les approches modernes:\n",
    "est-il possible d'extraire des tri-grammes de lettres pour représenter nos documents?\n",
    "\n",
    "Quelle performances attendrent? Quels sont les avantages et les inconvénients d'une telle approche?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèles de Machine Learning\n",
    "\n",
    "Avant de lancer de grandes expériences, il faut se construire une base de travail solide en étudiant les questions suivantes:\n",
    "\n",
    "- Combien de temps ça prend d'apprendre un classifieur NB/SVM/RegLog sur ces données en fonction de la taille du vocabulaire?\n",
    "- La validation croisée est-elle nécessaire? Est ce qu'on obtient les mêmes résultats avec un simple *split*?\n",
    "- La validation croisée est-elle stable? A partir de combien de fold (travailler avec différentes graines aléatoires et faire des statistiques basiques)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première campagne d'expériences\n",
    "\n",
    "Les techniques sur lesquelles nous travaillons étant sujettes au sur-apprentissage: trouver le paramètre de régularisation dans la documentation et optimiser ce paramètre au sens de la métrique qui vous semble la plus appropriée (cf question précédente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equilibrage des données\n",
    "\n",
    "Un problème reconnu comme dur dans la communauté est celui de l'équilibrage des classes (*balance* en anglais). Que faire si les données sont à 80, 90 ou 99% dans une des classes?\n",
    "Le problème est dur mais fréquent; les solutions sont multiples mais on peut isoler 3 grandes familles de solution.\n",
    "\n",
    "1. Ré-équilibrer le jeu de données: supprimer des données dans la classe majoritaire et/ou sur-échantilloner la classe minoritaire.<BR>\n",
    "   $\\Rightarrow$ A vous de jouer pour cette technique\n",
    "1. Changer la formulation de la fonction de coût pour pénaliser plus les erreurs dans la classe minoritaire:\n",
    "soit une fonction $\\Delta$ mesurant les écarts entre $f(x_i)$ et $y_i$ \n",
    "$$C = \\sum_i  \\alpha_i \\Delta(f(x_i),y_i), \\qquad \\alpha_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & \\mbox{si } y_i \\in \\mbox{classe majoritaire}\\\\\n",
    "B>1 & \\mbox{si } y_i \\in \\mbox{classe minoritaire}\\\\\n",
    "\\end{array} \\right.$$\n",
    "<BR>\n",
    "   $\\Rightarrow$ Les SVM et d'autres approches sklearn possèdent des arguments pour régler $B$ ou $1/B$... Ces arguments sont utiles mais pas toujours suffisant.\n",
    "1. Courbe ROC et modification du biais. Une fois la fonction $\\hat y = f(x)$ apprise, il est possible de la *bidouiller* a posteriori: si toutes les prédictions $\\hat y$ sont dans une classe, on va introduire $b$ dans $\\hat y = f(x) + b$ et le faire varier jusqu'à ce qu'un des points change de classe. On peut ensuite aller de plus en plus loin.\n",
    "Le calcul de l'ensemble des scores associés à cette approche mène directement à la courbe ROC.\n",
    "\n",
    "**Note:** certains classifieurs sont intrinsèquement plus résistante au problème d'équilibrage, c'est par exemple le cas des techniques de gradient boosting que vous verrez l'an prochain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
